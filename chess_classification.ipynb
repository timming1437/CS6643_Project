{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chess_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBgXpAya_gz",
        "outputId": "8d311a94-4bcd-4ea1-e76c-263f722c9035"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4UAVaJjfrKl",
        "outputId": "80d045e1-a85b-443a-d81f-124722bb52b8"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-p-E31bbshH"
      },
      "source": [
        "data_folder = '/content/drive/My Drive/cs6643final_project/data/'\n",
        "model_folder = '/content/drive/My Drive/cs6643final_project/model/'\n",
        "image_size = (224, 224)\n",
        "batch_size = 32"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMFWJxQxb-ll"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# use keras data generator to generate more images by randomly rotate and flip the image in our dataset.\n",
        "# set the range of rotation to be 30 degrees\n",
        "data_generator = ImageDataGenerator(\n",
        "        rotation_range=30,\n",
        "        rescale=1.0/255,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\")\n",
        "\n",
        "test_data_generator = ImageDataGenerator(rescale=1.0/255)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01-nKOFIotVF",
        "outputId": "4c338308-507c-4a4c-8a28-2612899fa0bb"
      },
      "source": [
        "train_data_gen = data_generator.flow_from_directory(\n",
        "    data_folder + \"train\",\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical',\n",
        "    color_mode = 'rgb',\n",
        "    shuffle=True,\n",
        "    interpolation = \"nearest\"  \n",
        ")\n",
        "\n",
        "test_data_gen = test_data_generator.flow_from_directory(\n",
        "    data_folder + \"test\",\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical',\n",
        "    color_mode = 'rgb',\n",
        "    shuffle=False     \n",
        ")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2156 images belonging to 13 classes.\n",
            "Found 260 images belonging to 13 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5VM_H8nmtT_"
      },
      "source": [
        "## try to classify the chess images by a simple cnn model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(13))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer= 'adam', loss = 'categorical_crossentropy', metrics= ['categorical_accuracy'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKDlYmwEdInF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572fc38a-0931-4e7b-df9f-6f3036acdf20"
      },
      "source": [
        "## train the simple cnn model\n",
        "epochs = 30\n",
        "\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    validation_data = test_data_gen\n",
        ")\n",
        "model.save_weights(model_folder + \"model_cnn.h5\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "68/68 [==============================] - 623s 9s/step - loss: 2.5070 - categorical_accuracy: 0.1206 - val_loss: 2.1921 - val_categorical_accuracy: 0.1462\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - 26s 386ms/step - loss: 2.2191 - categorical_accuracy: 0.1605 - val_loss: 2.0272 - val_categorical_accuracy: 0.2154\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - 26s 379ms/step - loss: 2.1196 - categorical_accuracy: 0.1814 - val_loss: 1.9763 - val_categorical_accuracy: 0.1923\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - 26s 381ms/step - loss: 2.0310 - categorical_accuracy: 0.2032 - val_loss: 1.9061 - val_categorical_accuracy: 0.2500\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - 26s 378ms/step - loss: 1.9761 - categorical_accuracy: 0.2064 - val_loss: 1.8601 - val_categorical_accuracy: 0.2808\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - 26s 379ms/step - loss: 1.9291 - categorical_accuracy: 0.2319 - val_loss: 1.8475 - val_categorical_accuracy: 0.2654\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - 26s 379ms/step - loss: 1.8983 - categorical_accuracy: 0.2393 - val_loss: 1.8121 - val_categorical_accuracy: 0.2769\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - 26s 377ms/step - loss: 1.8857 - categorical_accuracy: 0.2352 - val_loss: 1.7651 - val_categorical_accuracy: 0.2923\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - 26s 379ms/step - loss: 1.8362 - categorical_accuracy: 0.2430 - val_loss: 1.7925 - val_categorical_accuracy: 0.2615\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - 26s 377ms/step - loss: 1.8316 - categorical_accuracy: 0.2481 - val_loss: 1.7689 - val_categorical_accuracy: 0.2423\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - 26s 376ms/step - loss: 1.8033 - categorical_accuracy: 0.2556 - val_loss: 1.7414 - val_categorical_accuracy: 0.2923\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - 26s 377ms/step - loss: 1.7942 - categorical_accuracy: 0.2523 - val_loss: 1.6946 - val_categorical_accuracy: 0.3462\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - 26s 376ms/step - loss: 1.7715 - categorical_accuracy: 0.2672 - val_loss: 1.7317 - val_categorical_accuracy: 0.3231\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - 26s 379ms/step - loss: 1.7661 - categorical_accuracy: 0.2667 - val_loss: 1.6424 - val_categorical_accuracy: 0.3577\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - 25s 372ms/step - loss: 1.7272 - categorical_accuracy: 0.2718 - val_loss: 1.6236 - val_categorical_accuracy: 0.3615\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - 26s 375ms/step - loss: 1.6923 - categorical_accuracy: 0.2880 - val_loss: 1.5941 - val_categorical_accuracy: 0.3077\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - 25s 374ms/step - loss: 1.7003 - categorical_accuracy: 0.2945 - val_loss: 1.6561 - val_categorical_accuracy: 0.3615\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - 25s 372ms/step - loss: 1.6710 - categorical_accuracy: 0.2853 - val_loss: 1.5839 - val_categorical_accuracy: 0.3654\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - 26s 377ms/step - loss: 1.6926 - categorical_accuracy: 0.3061 - val_loss: 1.5772 - val_categorical_accuracy: 0.3115\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - 25s 372ms/step - loss: 1.6680 - categorical_accuracy: 0.3006 - val_loss: 1.5172 - val_categorical_accuracy: 0.3962\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - 25s 374ms/step - loss: 1.6410 - categorical_accuracy: 0.3029 - val_loss: 1.4894 - val_categorical_accuracy: 0.3654\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - 25s 374ms/step - loss: 1.6176 - categorical_accuracy: 0.3177 - val_loss: 1.4556 - val_categorical_accuracy: 0.3846\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - 25s 374ms/step - loss: 1.6624 - categorical_accuracy: 0.2862 - val_loss: 1.4453 - val_categorical_accuracy: 0.3615\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - 26s 375ms/step - loss: 1.6138 - categorical_accuracy: 0.3154 - val_loss: 1.4497 - val_categorical_accuracy: 0.3923\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - 25s 374ms/step - loss: 1.6060 - categorical_accuracy: 0.3177 - val_loss: 1.4017 - val_categorical_accuracy: 0.4269\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - 26s 377ms/step - loss: 1.5807 - categorical_accuracy: 0.3205 - val_loss: 1.4492 - val_categorical_accuracy: 0.3962\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - 25s 372ms/step - loss: 1.5954 - categorical_accuracy: 0.3168 - val_loss: 1.4184 - val_categorical_accuracy: 0.4192\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - 25s 373ms/step - loss: 1.5773 - categorical_accuracy: 0.3210 - val_loss: 1.4577 - val_categorical_accuracy: 0.3885\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - 25s 374ms/step - loss: 1.5706 - categorical_accuracy: 0.3302 - val_loss: 1.3883 - val_categorical_accuracy: 0.4192\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - 25s 373ms/step - loss: 1.5725 - categorical_accuracy: 0.3400 - val_loss: 1.3947 - val_categorical_accuracy: 0.4462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhi-cANGZhq0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abO8s9v3kxoK"
      },
      "source": [
        "## training a new model from starts cannot produce acceptable performance in terms of accuracy. The accuracy of the simple cnn model converges at around 0.4. Beyond 11 epoches, the model starts to be overfitting.\n",
        "\n",
        "## switch to utlizing transfer learning on top of a pretrained model \n",
        "from keras.models import Model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzXIuu_otQA6",
        "outputId": "ef1e14e2-9294-46ed-9bed-13d05774c784"
      },
      "source": [
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        "# dont include the three fully connected layers at the top in order to build our model.\n",
        "vgg = VGG16(weights=\"imagenet\", include_top = False, input_shape= (224,224,3))\n",
        "vgg.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az62NdKitjN0",
        "outputId": "2dd9dcda-992a-4dd5-8727-8ebc17cdce05"
      },
      "source": [
        "## load another pretrained model ResNet50\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "resnet = ResNet50(include_top = False, weights = \"imagenet\", input_shape = (224,224,3))\n",
        "\n",
        "resnet.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9qC4yrquJgM",
        "outputId": "57b8c8f1-afcb-4b85-9de4-f7a706f3daf7"
      },
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "vgg19 = VGG19(include_top = False, weights = \"imagenet\", input_shape = (224,224,3))\n",
        "\n",
        "vgg19.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAFBuWjjwObn"
      },
      "source": [
        "## freeze the pretrained models\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "for layer in vgg19.layers:\n",
        "  layer.trainable = False\n",
        "pretrained_models = [vgg, vgg19, resnet]\n",
        "final_models = []\n",
        "for model in pretrained_models:\n",
        "  ## take the output of the pretrained model as the input of our model\n",
        "  input  = model.output \n",
        "  ## flat the tensor output into 1D featuers\n",
        "  flatten = Flatten()(input)\n",
        "  ## add some fully connected layers on top of the flatten layer.\n",
        "  ## need to tune the number of fully connected layer as hyperparameter\n",
        "  ## number of nodes in the fully connected layers also need to be tuned\n",
        "  fc1 = Dense(500, activation = \"relu\")(flatten)\n",
        "  fc2 = Dense(500, activation = \"relu\")(fc1)\n",
        "  ## add a fully connected layer for the output prediction\n",
        "  output = Dense(13, activation = \"softmax\")(fc2)\n",
        "  modified_model = Model(inputs = model.input, outputs = output)\n",
        "  modified_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
        "  final_models.append(modified_model)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iug6EPWR1_Ur"
      },
      "source": [
        "## train the two modified models "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwdqON8m17MY",
        "outputId": "1b0bd595-6f6a-4da6-a5e7-e114fce2bed3"
      },
      "source": [
        "## train modified vgg 16 first\n",
        "epochs = 30\n",
        "\n",
        "history0 = final_models[0].fit(\n",
        "    train_data_gen,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    validation_data = test_data_gen\n",
        ")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "68/68 [==============================] - 1150s 16s/step - loss: 3.3548 - categorical_accuracy: 0.2035 - val_loss: 1.4954 - val_categorical_accuracy: 0.4538\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - 35s 519ms/step - loss: 1.1900 - categorical_accuracy: 0.5818 - val_loss: 0.9336 - val_categorical_accuracy: 0.6231\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - 36s 523ms/step - loss: 0.8318 - categorical_accuracy: 0.7115 - val_loss: 0.7871 - val_categorical_accuracy: 0.7000\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - 36s 521ms/step - loss: 0.5672 - categorical_accuracy: 0.7886 - val_loss: 0.8521 - val_categorical_accuracy: 0.7038\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - 35s 518ms/step - loss: 0.5197 - categorical_accuracy: 0.8102 - val_loss: 0.8243 - val_categorical_accuracy: 0.7000\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - 35s 520ms/step - loss: 0.4483 - categorical_accuracy: 0.8492 - val_loss: 0.7106 - val_categorical_accuracy: 0.7500\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - 35s 518ms/step - loss: 0.3364 - categorical_accuracy: 0.8786 - val_loss: 0.7309 - val_categorical_accuracy: 0.7615\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - 35s 519ms/step - loss: 0.2773 - categorical_accuracy: 0.9054 - val_loss: 0.7258 - val_categorical_accuracy: 0.7538\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - 35s 518ms/step - loss: 0.2687 - categorical_accuracy: 0.9094 - val_loss: 1.1038 - val_categorical_accuracy: 0.7038\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - 35s 520ms/step - loss: 0.2749 - categorical_accuracy: 0.9086 - val_loss: 0.8088 - val_categorical_accuracy: 0.7577\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - 35s 520ms/step - loss: 0.2448 - categorical_accuracy: 0.9140 - val_loss: 0.8661 - val_categorical_accuracy: 0.7423\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - 35s 519ms/step - loss: 0.2802 - categorical_accuracy: 0.9033 - val_loss: 0.6350 - val_categorical_accuracy: 0.7846\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - 35s 518ms/step - loss: 0.1905 - categorical_accuracy: 0.9303 - val_loss: 0.7876 - val_categorical_accuracy: 0.7500\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - 35s 517ms/step - loss: 0.1770 - categorical_accuracy: 0.9430 - val_loss: 0.8841 - val_categorical_accuracy: 0.7308\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - 35s 519ms/step - loss: 0.2052 - categorical_accuracy: 0.9295 - val_loss: 0.6640 - val_categorical_accuracy: 0.8038\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - 35s 517ms/step - loss: 0.1779 - categorical_accuracy: 0.9421 - val_loss: 0.9106 - val_categorical_accuracy: 0.7308\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - 35s 516ms/step - loss: 0.2407 - categorical_accuracy: 0.9249 - val_loss: 0.7561 - val_categorical_accuracy: 0.7692\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - 35s 516ms/step - loss: 0.1887 - categorical_accuracy: 0.9341 - val_loss: 0.6956 - val_categorical_accuracy: 0.7885\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - 35s 517ms/step - loss: 0.1688 - categorical_accuracy: 0.9376 - val_loss: 0.6580 - val_categorical_accuracy: 0.8000\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - 35s 515ms/step - loss: 0.1230 - categorical_accuracy: 0.9573 - val_loss: 1.0067 - val_categorical_accuracy: 0.7308\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - 35s 516ms/step - loss: 0.1668 - categorical_accuracy: 0.9397 - val_loss: 0.6363 - val_categorical_accuracy: 0.8038\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - 35s 522ms/step - loss: 0.1132 - categorical_accuracy: 0.9637 - val_loss: 0.6793 - val_categorical_accuracy: 0.8154\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - 35s 517ms/step - loss: 0.1210 - categorical_accuracy: 0.9645 - val_loss: 0.7015 - val_categorical_accuracy: 0.7885\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - 35s 517ms/step - loss: 0.1152 - categorical_accuracy: 0.9618 - val_loss: 0.9906 - val_categorical_accuracy: 0.7615\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - 35s 516ms/step - loss: 0.1149 - categorical_accuracy: 0.9564 - val_loss: 1.1015 - val_categorical_accuracy: 0.7423\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - 35s 517ms/step - loss: 0.2374 - categorical_accuracy: 0.9237 - val_loss: 0.8102 - val_categorical_accuracy: 0.7923\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - 35s 515ms/step - loss: 0.1206 - categorical_accuracy: 0.9598 - val_loss: 0.6803 - val_categorical_accuracy: 0.8231\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - 35s 518ms/step - loss: 0.1198 - categorical_accuracy: 0.9578 - val_loss: 0.6477 - val_categorical_accuracy: 0.8115\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - 35s 516ms/step - loss: 0.1244 - categorical_accuracy: 0.9616 - val_loss: 0.9776 - val_categorical_accuracy: 0.7769\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - 36s 520ms/step - loss: 0.0896 - categorical_accuracy: 0.9684 - val_loss: 0.8058 - val_categorical_accuracy: 0.8231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oXRzorrE2TH"
      },
      "source": [
        "## save the training history to file\n",
        "import numpy as np\n",
        "np.save(model_folder + 'history0.npy', history0.history)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4rt_7mgBEFP",
        "outputId": "eae2b599-9819-4ac0-b24c-13eb63194106"
      },
      "source": [
        "## train modified vgg19\n",
        "epochs = 30\n",
        "\n",
        "history1 = final_models[1].fit(\n",
        "    train_data_gen,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    validation_data = test_data_gen\n",
        ")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "68/68 [==============================] - 39s 554ms/step - loss: 3.9031 - categorical_accuracy: 0.1509 - val_loss: 1.7827 - val_categorical_accuracy: 0.4038\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - 38s 551ms/step - loss: 1.4391 - categorical_accuracy: 0.5088 - val_loss: 1.2604 - val_categorical_accuracy: 0.4885\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - 37s 540ms/step - loss: 0.9894 - categorical_accuracy: 0.6546 - val_loss: 1.0441 - val_categorical_accuracy: 0.5846\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - 37s 546ms/step - loss: 0.8092 - categorical_accuracy: 0.7125 - val_loss: 0.9199 - val_categorical_accuracy: 0.6615\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - 37s 548ms/step - loss: 0.6939 - categorical_accuracy: 0.7434 - val_loss: 0.9719 - val_categorical_accuracy: 0.6577\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - 37s 548ms/step - loss: 0.7219 - categorical_accuracy: 0.7544 - val_loss: 0.9687 - val_categorical_accuracy: 0.6846\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - 37s 549ms/step - loss: 0.5540 - categorical_accuracy: 0.8033 - val_loss: 0.7855 - val_categorical_accuracy: 0.7000\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - 37s 545ms/step - loss: 0.5317 - categorical_accuracy: 0.8121 - val_loss: 0.8001 - val_categorical_accuracy: 0.7269\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - 37s 544ms/step - loss: 0.5290 - categorical_accuracy: 0.8262 - val_loss: 0.6567 - val_categorical_accuracy: 0.7615\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - 37s 543ms/step - loss: 0.3898 - categorical_accuracy: 0.8540 - val_loss: 0.7343 - val_categorical_accuracy: 0.7346\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - 37s 546ms/step - loss: 0.3861 - categorical_accuracy: 0.8752 - val_loss: 0.7638 - val_categorical_accuracy: 0.6885\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - 37s 547ms/step - loss: 0.4311 - categorical_accuracy: 0.8435 - val_loss: 0.9055 - val_categorical_accuracy: 0.7346\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - 37s 548ms/step - loss: 0.3732 - categorical_accuracy: 0.8608 - val_loss: 0.9068 - val_categorical_accuracy: 0.6846\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - 37s 546ms/step - loss: 0.4244 - categorical_accuracy: 0.8612 - val_loss: 0.8296 - val_categorical_accuracy: 0.7115\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - 37s 545ms/step - loss: 0.3186 - categorical_accuracy: 0.8912 - val_loss: 0.8038 - val_categorical_accuracy: 0.7385\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - 37s 548ms/step - loss: 0.2765 - categorical_accuracy: 0.9195 - val_loss: 1.0405 - val_categorical_accuracy: 0.7000\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - 37s 544ms/step - loss: 0.2840 - categorical_accuracy: 0.9000 - val_loss: 0.5850 - val_categorical_accuracy: 0.8077\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.2708 - categorical_accuracy: 0.9117 - val_loss: 0.7252 - val_categorical_accuracy: 0.7769\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - 37s 544ms/step - loss: 0.3241 - categorical_accuracy: 0.8846 - val_loss: 0.7935 - val_categorical_accuracy: 0.7500\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - 37s 542ms/step - loss: 0.2476 - categorical_accuracy: 0.9104 - val_loss: 0.6819 - val_categorical_accuracy: 0.7692\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.2304 - categorical_accuracy: 0.9131 - val_loss: 0.7737 - val_categorical_accuracy: 0.7846\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - 37s 542ms/step - loss: 0.3039 - categorical_accuracy: 0.9000 - val_loss: 0.8959 - val_categorical_accuracy: 0.7269\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - 37s 543ms/step - loss: 0.2244 - categorical_accuracy: 0.9119 - val_loss: 1.0494 - val_categorical_accuracy: 0.7308\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - 37s 545ms/step - loss: 0.2535 - categorical_accuracy: 0.8976 - val_loss: 0.6657 - val_categorical_accuracy: 0.7808\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - 37s 547ms/step - loss: 0.2069 - categorical_accuracy: 0.9176 - val_loss: 0.7773 - val_categorical_accuracy: 0.7923\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.2047 - categorical_accuracy: 0.9274 - val_loss: 0.6599 - val_categorical_accuracy: 0.7731\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - 37s 540ms/step - loss: 0.2039 - categorical_accuracy: 0.9279 - val_loss: 0.9407 - val_categorical_accuracy: 0.7423\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - 37s 543ms/step - loss: 0.2055 - categorical_accuracy: 0.9266 - val_loss: 1.2495 - val_categorical_accuracy: 0.7038\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - 37s 538ms/step - loss: 0.2089 - categorical_accuracy: 0.9248 - val_loss: 1.5240 - val_categorical_accuracy: 0.6692\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - 37s 540ms/step - loss: 0.3689 - categorical_accuracy: 0.8819 - val_loss: 0.6452 - val_categorical_accuracy: 0.8269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3wS95frAeZI"
      },
      "source": [
        "np.save(model_folder + 'history1.npy', history1.history)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn18f5Ex8lH-"
      },
      "source": [
        "## The results shows that vgg16 has better "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcVRRS0lwKCG",
        "outputId": "ece15126-b2ab-4c21-e004-17d9da01cf8e"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "history2 = final_models[2].fit(\n",
        "    train_data_gen,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    validation_data = test_data_gen\n",
        ")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "68/68 [==============================] - 33s 435ms/step - loss: 8.9991 - categorical_accuracy: 0.0809 - val_loss: 2.5918 - val_categorical_accuracy: 0.0769\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - 28s 415ms/step - loss: 2.5852 - categorical_accuracy: 0.1005 - val_loss: 2.5928 - val_categorical_accuracy: 0.0769\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - 28s 414ms/step - loss: 2.5375 - categorical_accuracy: 0.1075 - val_loss: 2.5581 - val_categorical_accuracy: 0.0538\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - 28s 412ms/step - loss: 2.5178 - categorical_accuracy: 0.0952 - val_loss: 2.5093 - val_categorical_accuracy: 0.1577\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - 28s 412ms/step - loss: 2.4217 - categorical_accuracy: 0.1562 - val_loss: 2.4738 - val_categorical_accuracy: 0.1231\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - 28s 409ms/step - loss: 2.4933 - categorical_accuracy: 0.1226 - val_loss: 2.5705 - val_categorical_accuracy: 0.0769\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - 28s 409ms/step - loss: 2.5585 - categorical_accuracy: 0.0863 - val_loss: 2.6865 - val_categorical_accuracy: 0.0769\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - 28s 410ms/step - loss: 2.5523 - categorical_accuracy: 0.0826 - val_loss: 2.5830 - val_categorical_accuracy: 0.0769\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - 28s 412ms/step - loss: 2.5476 - categorical_accuracy: 0.0962 - val_loss: 2.5863 - val_categorical_accuracy: 0.0769\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - 28s 408ms/step - loss: 2.5360 - categorical_accuracy: 0.0935 - val_loss: 2.5915 - val_categorical_accuracy: 0.0769\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - 28s 412ms/step - loss: 2.5394 - categorical_accuracy: 0.0925 - val_loss: 2.5929 - val_categorical_accuracy: 0.0769\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - 28s 409ms/step - loss: 2.5321 - categorical_accuracy: 0.0965 - val_loss: 2.5946 - val_categorical_accuracy: 0.0769\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - 28s 409ms/step - loss: 2.5403 - categorical_accuracy: 0.0907 - val_loss: 2.5940 - val_categorical_accuracy: 0.0769\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - 28s 410ms/step - loss: 2.5402 - categorical_accuracy: 0.0993 - val_loss: 2.5958 - val_categorical_accuracy: 0.0769\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - 28s 410ms/step - loss: 2.5337 - categorical_accuracy: 0.0944 - val_loss: 2.5956 - val_categorical_accuracy: 0.0769\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - 28s 411ms/step - loss: 2.5488 - categorical_accuracy: 0.0882 - val_loss: 2.5961 - val_categorical_accuracy: 0.0769\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - 28s 408ms/step - loss: 2.5409 - categorical_accuracy: 0.0892 - val_loss: 2.5979 - val_categorical_accuracy: 0.0769\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - 28s 410ms/step - loss: 2.5406 - categorical_accuracy: 0.0878 - val_loss: 2.5973 - val_categorical_accuracy: 0.0769\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - 28s 410ms/step - loss: 2.5366 - categorical_accuracy: 0.0963 - val_loss: 2.5977 - val_categorical_accuracy: 0.0769\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - 28s 408ms/step - loss: 2.5393 - categorical_accuracy: 0.0948 - val_loss: 2.5974 - val_categorical_accuracy: 0.0769\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - 28s 408ms/step - loss: 2.5331 - categorical_accuracy: 0.0983 - val_loss: 2.5973 - val_categorical_accuracy: 0.0769\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - 28s 407ms/step - loss: 2.5427 - categorical_accuracy: 0.0958 - val_loss: 2.5964 - val_categorical_accuracy: 0.0769\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - 28s 408ms/step - loss: 2.5429 - categorical_accuracy: 0.0842 - val_loss: 2.5995 - val_categorical_accuracy: 0.0769\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - 28s 407ms/step - loss: 2.5414 - categorical_accuracy: 0.0911 - val_loss: 2.5981 - val_categorical_accuracy: 0.0769\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - 28s 407ms/step - loss: 2.5385 - categorical_accuracy: 0.0916 - val_loss: 2.5976 - val_categorical_accuracy: 0.0769\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - 28s 410ms/step - loss: 2.5405 - categorical_accuracy: 0.0964 - val_loss: 2.5979 - val_categorical_accuracy: 0.0769\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - 28s 405ms/step - loss: 2.5357 - categorical_accuracy: 0.0950 - val_loss: 2.5973 - val_categorical_accuracy: 0.0769\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - 28s 408ms/step - loss: 2.5989 - categorical_accuracy: 0.0736 - val_loss: 2.5953 - val_categorical_accuracy: 0.0769\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - 28s 405ms/step - loss: 2.5388 - categorical_accuracy: 0.0925 - val_loss: 2.5960 - val_categorical_accuracy: 0.0769\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - 28s 407ms/step - loss: 2.5321 - categorical_accuracy: 0.1026 - val_loss: 2.5966 - val_categorical_accuracy: 0.0769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh-YGJOjwGty"
      },
      "source": [
        "final_models[0].save_weights(model_folder + 'modified_vgg16.h5')\n",
        "final_models[1].save_weights(model_folder + 'modified_vgg19.h5')\n",
        "final_models[2].save_weights(model_folder  + 'modified_resnet.h5')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "5jCdIMnZE3lF",
        "outputId": "8a6f5c2b-4f4b-427e-c3fa-a1c396f545f3"
      },
      "source": [
        "## the result shows that resnet is not suitable for this task\n",
        "\n",
        "## compare the performance of vgg16 and vgg19 with validation accuracy\n",
        "import seaborn as sea\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.title(\"Accuracy of modified vgg16 vs Accuracy of modified vgg19\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Categorial Accuracy\")\n",
        "plt.plot(history0.history['categorical_accuracy'], '-r', label = 'vgg16')\n",
        "plt.plot(history1.history['categorical_accuracy'], '--b', label = 'vgg19')\n",
        "plt.legend([\"modified vgg16\", \"modified vgg19\"])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f72286a9e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFma9XViAjEd"
      },
      "source": [
        "## load model parameters\n",
        "final_models[0].load_weights(model_folder + \"modified_vgg16.h5\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqYm6wykGDjA",
        "outputId": "e44e9a08-5894-485d-c620-f3e7d944c6a5"
      },
      "source": [
        "## chose vgg16 as our choice\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "labels = ['bb', 'bk', 'bn', 'bp', 'bq', 'br', 'empty', 'wb', 'wk', 'wn', 'wp', 'wq', 'wr']\n",
        "test_data_gen.reset()\n",
        "prediction = final_models[0].predict(test_data_gen)\n",
        "y = np.argmax(prediction, axis = -1)\n",
        "true_labels = test_data_gen.classes[test_data_gen.index_array]\n",
        "\n",
        "cm = confusion_matrix(true_labels, y)\n",
        "report = classification_report(true_labels, y, target_names = labels)\n",
        "\n",
        "print(\"The confusion matrix of the prediction of modified vgg16\")\n",
        "print(cm)\n",
        "\n",
        "print(\"The classification report of the prediction of modified vgg16\")\n",
        "print(report)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The confusion matrix of the prediction of modified vgg16\n",
            "[[18  0  0  1  0  0  0  0  0  0  0  0  1]\n",
            " [ 1 17  0  0  1  1  0  0  0  0  0  0  0]\n",
            " [ 1  1 16  0  0  2  0  0  0  0  0  0  0]\n",
            " [ 4  0  0 16  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0 14  0  0  0  0  0  0  0  1]\n",
            " [ 2  0  1  0  0 17  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 19  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 17  0  0  1  0  2]\n",
            " [ 0  0  0  0  0  0  0  2 12  0  0  4  2]\n",
            " [ 0  0  0  0  0  0  0  1  0 14  1  0  4]\n",
            " [ 0  0  0  0  0  0  0  2  0  0 18  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 19  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1  1 18]]\n",
            "The classification report of the prediction of modified vgg16\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          bb       0.58      0.90      0.71        20\n",
            "          bk       0.94      0.85      0.89        20\n",
            "          bn       0.94      0.80      0.86        20\n",
            "          bp       0.94      0.80      0.86        20\n",
            "          bq       0.93      0.70      0.80        20\n",
            "          br       0.85      0.85      0.85        20\n",
            "       empty       1.00      0.95      0.97        20\n",
            "          wb       0.77      0.85      0.81        20\n",
            "          wk       1.00      0.60      0.75        20\n",
            "          wn       0.93      0.70      0.80        20\n",
            "          wp       0.86      0.90      0.88        20\n",
            "          wq       0.79      0.95      0.86        20\n",
            "          wr       0.62      0.90      0.73        20\n",
            "\n",
            "    accuracy                           0.83       260\n",
            "   macro avg       0.86      0.83      0.83       260\n",
            "weighted avg       0.86      0.83      0.83       260\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}